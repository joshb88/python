{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joshua Boehm</br>\n",
    "December 1, 2022</br>\n",
    "Math 3423</br>\n",
    "Homework 4</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries/Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from numpy.linalg import solve\n",
    "from numpy.linalg import qr\n",
    "from numpy.linalg import svd\n",
    "from scipy.linalg import diagsvd\n",
    "from scipy.linalg import pinv\n",
    "from scipy.optimize import minimize\n",
    "from statistics import mean \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data points\n",
    "$$(2,3), (3,2), (5,1), (6,0)$$\n",
    "derive the equation of the least-squares line\n",
    "$$y = mx + b$$\n",
    "that best fits the given data points. Solve the system using the QR factorization, the SVD, and the Normal Equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.matrix([[2,1],[3,1],[5,1],[6,1]])\n",
    "b = np.matrix([3,2,1,0]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QR Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix created from the data points:\n",
    "$$\\begin{bmatrix}2, 1 \\\\ 3, 1 \\\\5, 1 \\\\6, 1\\end{bmatrix}\n",
    "\\begin{bmatrix}m \\\\ b\\end{bmatrix}=\\begin{bmatrix}3 \\\\2 \\\\1 \\\\0\\end{bmatrix}$$\n",
    "\n",
    "To solve using $QR$ factorization, we first factor $A$ as $QR$. <br>\n",
    "Next, we pre-multiply with $Q^T$, giving $Rx=Q^Tb$ ($Q^TQ$ is the identity matrix)<br>\n",
    "Now, we can call `solve()` with $R$ and $Q^Tb$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.7]\n",
      " [ 4.3]]\n"
     ]
    }
   ],
   "source": [
    "Q, R = qr(A)\n",
    "\n",
    "x = solve(R, Q.T @ b)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x = \\begin{bmatrix} -0.7 \\\\ 4.3 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve using $SVD$ factorization, we first factor $A$ as $SVD$.    \n",
    "Now, we create the pseudo-inverse by taking the transpose of the the parts in reverse order, except with $\\Sigma$, we take the take the  reciprocals before transposing.    \n",
    "Last, we can call pre-multiply $b$ with $A^\\dagger$, as $A^\\dagger A$ is the identity matrix\n",
    "\n",
    "$$\n",
    "Ax = b \\\\\n",
    "U \\Sigma V^T x = b \\\\\n",
    "A^\\dagger = V \\Sigma^\\dagger U^T \\\\\n",
    "A^TA = I \\\\\n",
    "Ix = A^\\dagger b \\\\\n",
    "x = A^\\dagger b \\space\\space \\text{or} \\space\\space V \\Sigma^\\dagger U^T b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.65  0.45  0.05 -0.15]\n",
      " [ 0.45  0.35  0.15  0.05]\n",
      " [ 0.05  0.15  0.35  0.45]\n",
      " [-0.15  0.05  0.45  0.65]]\n",
      "[[ 1.00000000e+00 -5.55111512e-17]\n",
      " [-2.22044605e-15  1.00000000e+00]]\n",
      "[[-0.7]\n",
      " [ 4.3]]\n"
     ]
    }
   ],
   "source": [
    "U, sigma, VT = svd(A)\n",
    "Sigma = diagsvd(sigma, len(A), len(A.T))\n",
    "PseudoA = (VT).T @ pinv(Sigma) @ U.T \n",
    "print(A @ PseudoA)\n",
    "print(PseudoA @ A)\n",
    "x = PseudoA @  b\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x = \\begin{bmatrix} -0.7 \\\\ 4.3 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal equation is that which minimizes the sum of the square differences between the left and right sides:<br>\n",
    "$$\n",
    "A^TAx = A^Tb\n",
    "$$\n",
    "\n",
    "In this case, I would just use `solve(A.T @ A, A.T @ b)` and see. You could utilize QR factorizations (or any other for that matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.7]\n",
      " [ 4.3]]\n",
      "\n",
      "[[-0.7]\n",
      " [ 4.3]]\n",
      "\n",
      "[[-0.7]\n",
      " [ 4.3]]\n"
     ]
    }
   ],
   "source": [
    "solvex = solve(A.T @ A, A.T @ b)\n",
    "solveqr = solve(R.T @ Q.T @ Q @ R, R.T @ Q.T @ b)\n",
    "solvesvd = solve(VT.T @ Sigma.T @ U.T @ U @ Sigma @ VT, VT.T @ Sigma.T @ U.T @ b)\n",
    "\n",
    "print(f\"{solvex}\\n\\n{solveqr}\\n\\n{solvesvd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1279a65f0>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X1 = np.array(A[:,0])\n",
    "Y1 = np.array(b)\n",
    "x = np.linspace(0,7,35)\n",
    "y = (-0.7)*x+(4.3)\n",
    "plt.scatter(X1,Y1, edgecolor='k',c='none',s=50)\n",
    "plt.plot(x,y, \"r--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use functions from `scipy.optimize.minimize` to minimize:<br><br>\n",
    "The Rosenbrock function\n",
    "$$\n",
    "f(x_1,x_2)=(7−x_1)^2 +100(x_2 −x^2_1)^2 +10\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Rosenbrock function is defined as:\n",
    "$$f(x,y)=(a-x)^{2}+b(y-x^{2})^{2} + c$$\n",
    "\n",
    "In the case of our example, $a = 7$, $b=100$, and $c = 10$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to derive the gradiant, let's first define it:\n",
    "$$\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\\\ \\vdots\\end{bmatrix} \\quad \\text{OR} \\quad \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots\\end{bmatrix}$$\n",
    "\n",
    "First with regards to $x_1$:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_1} (7−x_1)^2 +100(x_2 −x^2_1)^2 +10 = 400x_1^3-400x_1x_2 +2x_1-14$$\n",
    "\n",
    "Next with regards to $x_2$:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_2} (7−x_1)^2 +100(x_2 −x^2_1)^2 +10 = 200x_2-200x_1^2$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\\nabla f = \\begin{bmatrix} 400x_1^3-400x_1x_2 +2x_1-14 \\\\ 200x_2-200x_1^2\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, to derive the Hessian, it might help to define it.\n",
    "\n",
    "$$\\bf H_f = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2f}{\\partial x_1^2} & \\frac{\\partial^2f}{\\partial x_1\\partial x_2} \\\\\n",
    "\\frac{\\partial^2f}{\\partial x_2\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "So our second partials are as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial^2f}{\\partial x_1^2} &= 1200x_1^2-400x_2+2 \\\\\n",
    "\\frac{\\partial^2f}{\\partial x_2\\partial x_1} &= -400x_1 \\\\\n",
    "\\frac{\\partial^2f}{\\partial x_2\\partial x_1} &=  -400x_1\\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_2^2} &= 200\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "This makes the Hessian:\n",
    "$$\n",
    "\\bf {H}_f = \\begin{bmatrix} 1200x_1^2-400x_2+2 &-400x_1 \\\\ -400x_1 &200\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 7; b = 100; c = 10\n",
    "def f(x): \n",
    "   return (a - x[0])**2 + b*(x[1] - x[0]**2)**2 + c\n",
    "\n",
    "def gradient(x):\n",
    "   return np.array([-2*(a - x[0]) - 4.*b*x[0]*(x[1] - x[0]**2), \n",
    "                    2*b*(x[1] - x[0]**2)])\n",
    "\n",
    "def hessian(x):\n",
    "   return np.array([[2 - 4*b*x[1] + 12*b*x[0]**2, -4*b*x[0] ], \n",
    "                 [-4*b*x[0], 2*b]])\n",
    "\n",
    "optimization_methods = {'newton-cg':{'function evals': [], 'function iters': []}, \n",
    "'nelder-mead':{'function evals': [], 'function iters': []}, \n",
    "'powell':{'function evals': [], 'function iters': []},\n",
    "'bfgs':{'function evals': [], 'function iters': []}, \n",
    "'dogleg':{'function evals': [], 'function iters': []}}\n",
    "\n",
    "starting_points = [np.random.randint(-5,5, size = (1,2)) for i in range(3)]\n",
    "section_break = \"==========================================================\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: <span style=\"color: tan;\">Newton-CG</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "Test Run 1 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[1 1]]\n",
      "The Minimum Occurs at (x, y) =  [ 7.         48.99999997]\n",
      "The Minimum Value =  10.0\n",
      "Other Statistics:\n",
      "     fun: 10.0\n",
      "     jac: array([ 2.32592723e-05, -1.66156298e-06])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 222\n",
      "    nhev: 114\n",
      "     nit: 114\n",
      "    njev: 222\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([ 7.        , 48.99999997])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "==========================================================\n",
      "Test Run 2 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[1 0]]\n",
      "The Minimum Occurs at (x, y) =  [ 6.99967685 48.99547583]\n",
      "The Minimum Value =  10.0\n",
      "Other Statistics:\n",
      "     fun: 10.000000104428656\n",
      "     jac: array([ 0.00664118, -0.00052053])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 219\n",
      "    nhev: 109\n",
      "     nit: 109\n",
      "    njev: 219\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([ 6.99967685, 48.99547583])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "==========================================================\n",
      "Test Run 3 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[-1 -2]]\n",
      "The Minimum Occurs at (x, y) =  [ 6.99999994 48.99999913]\n",
      "The Minimum Value =  10.0\n",
      "Other Statistics:\n",
      "     fun: 10.000000000000004\n",
      "     jac: array([ 7.50576764e-04, -5.36187400e-05])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 218\n",
      "    nhev: 118\n",
      "     nit: 118\n",
      "    njev: 218\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([ 6.99999994, 48.99999913])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "Summary Statistics of the Method:\n",
      "\n",
      "Function Evaluations:\n",
      "Minimum Function Evaluations: 218\n",
      "Maximum Function Evaluations: 222\n",
      "Mean Function Evaluations: 219.66666666666666\n",
      "\n",
      "Function Iterations:\n",
      "Minimum Function Iterations:  109\n",
      "Maximum Function Iterations:  118\n",
      "Mean Function Iterations:  113.66666666666667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in enumerate(starting_points):\n",
    "    result = minimize(f, x[1], method = 'Newton-CG',jac = gradient, hess = hessian, tol = 1.e-7) \n",
    "    \n",
    "    print(section_break)\n",
    "    print('Test Run', x[0] + 1, ':')\n",
    "    print(section_break)\n",
    "\n",
    "    print('Starting Value Used: ', x[1])\n",
    "    print(\"The Minimum Occurs at (x, y) = \", result.x)\n",
    "    print(\"The Minimum Value = \", f(result.x).round(3))\n",
    "    \n",
    "    print(\"Other Statistics:\") \n",
    "    print(result) \n",
    "    print(section_break)\n",
    "    print(section_break)\n",
    "    print('\\n')\n",
    "\n",
    "   \n",
    "    optimization_methods['newton-cg']['function evals'].append(result['nfev'])\n",
    "    optimization_methods['newton-cg']['function iters'].append(result['nit'])\n",
    "    \n",
    "\n",
    "# Displaying a summary of both function evaluations and iterations\n",
    "print(f\"\\\n",
    "Summary Statistics of the Method:\\n\\n\\\n",
    "Function Evaluations:\\n\\\n",
    "Minimum Function Evaluations: {min(optimization_methods['newton-cg']['function evals'])}\\n\\\n",
    "Maximum Function Evaluations: {max(optimization_methods['newton-cg']['function evals'])}\\n\\\n",
    "Mean Function Evaluations: {mean(optimization_methods['newton-cg']['function evals'])}\\n\\n\\\n",
    "Function Iterations:\\n\\\n",
    "Minimum Function Iterations:  {min(optimization_methods['newton-cg']['function iters'])}\\n\\\n",
    "Maximum Function Iterations:  {max(optimization_methods['newton-cg']['function iters'])}\\n\\\n",
    "Mean Function Iterations:  {mean(optimization_methods['newton-cg']['function iters'])}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: <span style=\"color: tan;\">Nelder-Mead</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "Test Run 1 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[1 1]]\n",
      "The Minimum Occurs at (x, y) =  [ 6.99996663 48.99953487]\n",
      "The Minimum Value =  10.0\n",
      "Other Statistics:\n",
      " final_simplex: (array([[ 6.99996663, 48.99953487],\n",
      "       [ 7.000038  , 49.00053481],\n",
      "       [ 7.00002285, 49.00032668]]), array([10.        , 10.        , 10.00000001]))\n",
      "           fun: 10.000000001523516\n",
      "       message: 'Optimization terminated successfully.'\n",
      "          nfev: 253\n",
      "           nit: 133\n",
      "        status: 0\n",
      "       success: True\n",
      "             x: array([ 6.99996663, 48.99953487])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "==========================================================\n",
      "Test Run 2 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[1 0]]\n",
      "The Minimum Occurs at (x, y) =  [ 7.00001556 49.00022085]\n",
      "The Minimum Value =  10.0\n",
      "Other Statistics:\n",
      " final_simplex: (array([[ 7.00001556, 49.00022085],\n",
      "       [ 6.9999772 , 48.99967764],\n",
      "       [ 7.00004373, 49.00060994]]), array([10., 10., 10.]))\n",
      "           fun: 10.000000001190658\n",
      "       message: 'Optimization terminated successfully.'\n",
      "          nfev: 385\n",
      "           nit: 211\n",
      "        status: 0\n",
      "       success: True\n",
      "             x: array([ 7.00001556, 49.00022085])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "==========================================================\n",
      "Test Run 3 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[-1 -2]]\n",
      "The Minimum Occurs at (x, y) =  [ 7.00001786 49.00025349]\n",
      "The Minimum Value =  10.0\n",
      "Other Statistics:\n",
      " final_simplex: (array([[ 7.00001786, 49.00025349],\n",
      "       [ 6.99999473, 48.99992137],\n",
      "       [ 7.00007542, 49.00105389]]), array([10.        , 10.        , 10.00000001]))\n",
      "           fun: 10.000000001464135\n",
      "       message: 'Optimization terminated successfully.'\n",
      "          nfev: 307\n",
      "           nit: 162\n",
      "        status: 0\n",
      "       success: True\n",
      "             x: array([ 7.00001786, 49.00025349])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "Summary Statistics of the Method:\n",
      "\n",
      "Function Evaluations:\n",
      "Minimum Function Evaluations: 253\n",
      "Maximum Function Evaluations: 385\n",
      "Mean Function Evaluations: 315\n",
      "\n",
      "Function Iterations:\n",
      "Minimum Function Iterations:  133\n",
      "Maximum Function Iterations:  211\n",
      "Mean Function Iterations:  168.66666666666666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in enumerate(starting_points):\n",
    "    result = minimize(f, x[1], method = 'Nelder-Mead',tol = 1.e-3) \n",
    "    \n",
    "    print(section_break)\n",
    "    print('Test Run', x[0] + 1, ':')\n",
    "    print(section_break)\n",
    "\n",
    "    print('Starting Value Used: ', x[1])\n",
    "    print(\"The Minimum Occurs at (x, y) = \", result.x)\n",
    "    print(\"The Minimum Value = \", f(result.x).round(3))\n",
    "    \n",
    "    print(\"Other Statistics:\") \n",
    "    print(result) \n",
    "\n",
    "    print(section_break)\n",
    "    print(section_break)\n",
    "    print('\\n')\n",
    "\n",
    "    optimization_methods['nelder-mead']['function evals'].append(result['nfev'])\n",
    "    optimization_methods['nelder-mead']['function iters'].append(result['nit'])\n",
    "\n",
    "# Displaying a summary of both function evaluations and iterations\n",
    "print(f\"\\\n",
    "Summary Statistics of the Method:\\n\\n\\\n",
    "Function Evaluations:\\n\\\n",
    "Minimum Function Evaluations: {min(optimization_methods['nelder-mead']['function evals'])}\\n\\\n",
    "Maximum Function Evaluations: {max(optimization_methods['nelder-mead']['function evals'])}\\n\\\n",
    "Mean Function Evaluations: {mean(optimization_methods['nelder-mead']['function evals'])}\\n\\n\\\n",
    "Function Iterations:\\n\\\n",
    "Minimum Function Iterations:  {min(optimization_methods['nelder-mead']['function iters'])}\\n\\\n",
    "Maximum Function Iterations:  {max(optimization_methods['nelder-mead']['function iters'])}\\n\\\n",
    "Mean Function Iterations:  {mean(optimization_methods['nelder-mead']['function iters'])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: <span style=\"color: tan;\">Powell</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "Test Run 1 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[1 1]]\n",
      "The Minimum Occurs at (x, y) =  [ 6.99994136 48.99911248]\n",
      "The Minimum Value =  10.0\n",
      "Other Statistics:\n",
      "   direc: array([[0.12291486, 1.68509597],\n",
      "       [0.06505741, 0.91022215]])\n",
      "     fun: 10.000000445882268\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 1114\n",
      "     nit: 38\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([ 6.99994136, 48.99911248])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "==========================================================\n",
      "Test Run 2 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[1 0]]\n",
      "The Minimum Occurs at (x, y) =  [ 6.99913231 48.98755538]\n",
      "The Minimum Value =  10.0\n",
      "Other Statistics:\n",
      "   direc: array([[0.12732975, 1.74179846],\n",
      "       [0.07352777, 1.02705118]])\n",
      "     fun: 10.000009615558303\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 1238\n",
      "     nit: 42\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([ 6.99913231, 48.98755538])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "==========================================================\n",
      "Test Run 3 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[-1 -2]]\n",
      "The Minimum Occurs at (x, y) =  [ 7.00000207 49.00000856]\n",
      "The Minimum Value =  10.0\n",
      "Other Statistics:\n",
      "   direc: array([[0.12132559, 1.66506049],\n",
      "       [0.06024862, 0.84352643]])\n",
      "     fun: 10.000000041694891\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 1201\n",
      "     nit: 41\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([ 7.00000207, 49.00000856])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "Summary Statistics of the Method:\n",
      "\n",
      "Function Evaluations:\n",
      "Minimum Function Evaluations: 1114\n",
      "Maximum Function Evaluations: 1238\n",
      "Mean Function Evaluations: 1184.3333333333333\n",
      "\n",
      "Function Iterations:\n",
      "Minimum Function Iterations:  38\n",
      "Maximum Function Iterations:  42\n",
      "Mean Function Iterations:  40.333333333333336\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in enumerate(starting_points):\n",
    "    result = minimize(f, x[1], method = 'Powell',tol = 1.e-4) \n",
    "    \n",
    "    print(section_break)\n",
    "    print('Test Run', x[0] + 1, ':')\n",
    "    print(section_break)\n",
    "\n",
    "    print('Starting Value Used: ', x[1])\n",
    "    print(\"The Minimum Occurs at (x, y) = \", result.x)\n",
    "    print(\"The Minimum Value = \", f(result.x).round(3))\n",
    "    \n",
    "    print(\"Other Statistics:\") \n",
    "    print(result) \n",
    "\n",
    "    print(section_break)\n",
    "    print(section_break)\n",
    "    print('\\n')\n",
    "\n",
    "    optimization_methods['powell']['function evals'].append(result['nfev'])\n",
    "    optimization_methods['powell']['function iters'].append(result['nit'])\n",
    "\n",
    "# Displaying a summary of both function evaluations and iterations\n",
    "print(f\"\\\n",
    "Summary Statistics of the Method:\\n\\n\\\n",
    "Function Evaluations:\\n\\\n",
    "Minimum Function Evaluations: {min(optimization_methods['powell']['function evals'])}\\n\\\n",
    "Maximum Function Evaluations: {max(optimization_methods['powell']['function evals'])}\\n\\\n",
    "Mean Function Evaluations: {mean(optimization_methods['powell']['function evals'])}\\n\\n\\\n",
    "Function Iterations:\\n\\\n",
    "Minimum Function Iterations:  {min(optimization_methods['powell']['function iters'])}\\n\\\n",
    "Maximum Function Iterations:  {max(optimization_methods['powell']['function iters'])}\\n\\\n",
    "Mean Function Iterations:  {mean(optimization_methods['powell']['function iters'])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 4: <span style=\"color: tan;\">BFGS</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "Test Run 1 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[1 1]]\n",
      "The Minimum Occurs at (x, y) =  [ 6.99984377 48.99781287]\n",
      "The Minimum Value =  10.0\n",
      "Other Statistics:\n",
      "      fun: 10.000000024406553\n",
      " hess_inv: array([[ 0.17059882,  2.38800512],\n",
      "       [ 2.38800512, 33.43163883]])\n",
      "      jac: array([-4.06503677e-05,  2.98023224e-06])\n",
      "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
      "     nfev: 324\n",
      "      nit: 59\n",
      "     njev: 104\n",
      "   status: 2\n",
      "  success: False\n",
      "        x: array([ 6.99984377, 48.99781287])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "==========================================================\n",
      "Test Run 2 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[1 0]]\n",
      "The Minimum Occurs at (x, y) =  [ 6.99984362 48.99781077]\n",
      "The Minimum Value =  10.0\n",
      "Other Statistics:\n",
      "      fun: 10.00000002445317\n",
      " hess_inv: array([[ 0.50991738,  7.1385856 ],\n",
      "       [ 7.1385856 , 99.94162214]])\n",
      "      jac: array([-1.1920929e-07,  0.0000000e+00])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 276\n",
      "      nit: 69\n",
      "     njev: 92\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([ 6.99984362, 48.99781077])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "==========================================================\n",
      "Test Run 3 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[-1 -2]]\n",
      "The Minimum Occurs at (x, y) =  [ 6.99984314 48.99780403]\n",
      "The Minimum Value =  10.0\n",
      "Other Statistics:\n",
      "      fun: 10.000000024603802\n",
      " hess_inv: array([[ 0.48714862,  6.81941886],\n",
      "       [ 6.81941886, 95.46758068]])\n",
      "      jac: array([ 8.10623169e-06, -5.96046448e-07])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 315\n",
      "      nit: 79\n",
      "     njev: 105\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([ 6.99984314, 48.99780403])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "Summary Statistics of the Method:\n",
      "\n",
      "Function Evaluations:\n",
      "Minimum Function Evaluations: 276\n",
      "Maximum Function Evaluations: 324\n",
      "Mean Function Evaluations: 305\n",
      "\n",
      "Function Iterations:\n",
      "Minimum Function Iterations:  59\n",
      "Maximum Function Iterations:  79\n",
      "Mean Function Iterations:  69\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in enumerate(starting_points):\n",
    "    result = minimize(f, x[1], method = 'BFGS',tol = 1.e-5) \n",
    "    \n",
    "    print(section_break)\n",
    "    print('Test Run', x[0] + 1, ':')\n",
    "    print(section_break)\n",
    "\n",
    "    print('Starting Value Used: ', x[1])\n",
    "    print(\"The Minimum Occurs at (x, y) = \", result.x)\n",
    "    print(\"The Minimum Value = \", f(result.x).round(3))\n",
    "    \n",
    "    print(\"Other Statistics:\") \n",
    "    print(result) \n",
    "\n",
    "    print(section_break)\n",
    "    print(section_break)\n",
    "    print('\\n')\n",
    "\n",
    "    optimization_methods['bfgs']['function evals'].append(result['nfev'])\n",
    "    optimization_methods['bfgs']['function iters'].append(result['nit'])\n",
    "\n",
    "# Displaying a summary of both function evaluations and iterations\n",
    "print(f\"\\\n",
    "Summary Statistics of the Method:\\n\\n\\\n",
    "Function Evaluations:\\n\\\n",
    "Minimum Function Evaluations: {min(optimization_methods['bfgs']['function evals'])}\\n\\\n",
    "Maximum Function Evaluations: {max(optimization_methods['bfgs']['function evals'])}\\n\\\n",
    "Mean Function Evaluations: {mean(optimization_methods['bfgs']['function evals'])}\\n\\n\\\n",
    "Function Iterations:\\n\\\n",
    "Minimum Function Iterations:  {min(optimization_methods['bfgs']['function iters'])}\\n\\\n",
    "Maximum Function Iterations:  {max(optimization_methods['bfgs']['function iters'])}\\n\\\n",
    "Mean Function Iterations:  {mean(optimization_methods['bfgs']['function iters'])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 5: <span style=\"color: tan;\">Dogleg</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "Test Run 1 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[1 1]]\n",
      "The Minimum Occurs at (x, y) =  [ 7.         48.99999999]\n",
      "The Minimum Value =  10.0\n",
      "Other Statistics:\n",
      "     fun: 10.0\n",
      "    hess: array([[39201.99999595, -2799.99999986],\n",
      "       [-2799.99999986,   200.        ]])\n",
      "     jac: array([ 2.71501008e-08, -1.99094075e-09])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 41\n",
      "    nhev: 37\n",
      "     nit: 40\n",
      "    njev: 38\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([ 7.        , 48.99999999])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "==========================================================\n",
      "Test Run 2 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[1 0]]\n",
      "The Minimum Occurs at (x, y) =  [ 7. 49.]\n",
      "The Minimum Value =  10.0\n",
      "Other Statistics:\n",
      "     fun: 10.0\n",
      "    hess: array([[39201.99999973, -2799.99999999],\n",
      "       [-2799.99999999,   200.        ]])\n",
      "     jac: array([ 5.31693214e-08, -3.80140364e-09])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 42\n",
      "    nhev: 37\n",
      "     nit: 41\n",
      "    njev: 38\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([ 7., 49.])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "==========================================================\n",
      "Test Run 3 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[-1 -2]]\n",
      "The Minimum Occurs at (x, y) =  [0.63995989 0.43895176]\n",
      "The Minimum Value =  50.537\n",
      "Other Statistics:\n",
      "     fun: 50.5365644483642\n",
      "    hess: array([[ 317.87768742, -255.98395611],\n",
      "       [-255.98395611,  200.        ]])\n",
      "     jac: array([-20.24680304,   5.88062075])\n",
      " message: 'A linalg error occurred, such as a non-psd Hessian.'\n",
      "    nfev: 9\n",
      "    nhev: 7\n",
      "     nit: 8\n",
      "    njev: 7\n",
      "  status: 3\n",
      " success: False\n",
      "       x: array([0.63995989, 0.43895176])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "Summary Statistics of the Method:\n",
      "\n",
      "Function Evaluations:\n",
      "Minimum Function Evaluations: 9\n",
      "Maximum Function Evaluations: 42\n",
      "Mean Function Evaluations: 30.666666666666668\n",
      "\n",
      "Function Iterations:\n",
      "Minimum Function Iterations:  8\n",
      "Maximum Function Iterations:  41\n",
      "Mean Function Iterations:  29.666666666666668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in enumerate(starting_points):\n",
    "    result = minimize(f, x[1], method = 'dogleg',jac = gradient, hess = hessian, tol = 1.e-7) \n",
    "    \n",
    "    print(section_break)\n",
    "    print('Test Run', x[0] + 1, ':')\n",
    "    print(section_break)\n",
    "\n",
    "    print('Starting Value Used: ', x[1])\n",
    "    print(\"The Minimum Occurs at (x, y) = \", result.x)\n",
    "    print(\"The Minimum Value = \", f(result.x).round(3))\n",
    "    \n",
    "    print(\"Other Statistics:\") \n",
    "    print(result) \n",
    "    print(section_break)\n",
    "    print(section_break)\n",
    "    print('\\n')\n",
    "\n",
    "   \n",
    "    optimization_methods['dogleg']['function evals'].append(result['nfev'])\n",
    "    optimization_methods['dogleg']['function iters'].append(result['nit'])\n",
    "    \n",
    "\n",
    "# Displaying a summary of both function evaluations and iterations\n",
    "print(f\"\\\n",
    "Summary Statistics of the Method:\\n\\n\\\n",
    "Function Evaluations:\\n\\\n",
    "Minimum Function Evaluations: {min(optimization_methods['dogleg']['function evals'])}\\n\\\n",
    "Maximum Function Evaluations: {max(optimization_methods['dogleg']['function evals'])}\\n\\\n",
    "Mean Function Evaluations: {mean(optimization_methods['dogleg']['function evals'])}\\n\\n\\\n",
    "Function Iterations:\\n\\\n",
    "Minimum Function Iterations:  {min(optimization_methods['dogleg']['function iters'])}\\n\\\n",
    "Maximum Function Iterations:  {max(optimization_methods['dogleg']['function iters'])}\\n\\\n",
    "Mean Function Iterations:  {mean(optimization_methods['dogleg']['function iters'])}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the `dogleg` method is the most efficient method at find the minimum with fewest of every summary statistic escribed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use functions from `scipy.optimize.minimize` to minimize:<br><br>\n",
    "The Booth function\n",
    "$$\n",
    "f(x_1,x_2)=(x_1 +2x_2 −7)^2 +(2x_1 +x_2 −5)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the function is defined as it's base form. We need only to find the Gradiant and the Hessian for the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the definition of the gradiant is:\n",
    "$$\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\\\ \\vdots\\end{bmatrix} \\quad \\text{OR} \\quad \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots\\end{bmatrix}$$\n",
    "\n",
    "First with regards to $x_1$:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_1} (x_1 +2x_2 −7)^2 +(2x_1 +x_2 −5)^2 = 10x_1+8x_2-34$$\n",
    "\n",
    "Next with regards to $x_2$:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_2} (x_1 +2x_2 −7)^2 +(2x_1 +x_2 −5)^2=8x_1+10x_2-38$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\\nabla f = \\begin{bmatrix} 10x_1+8x_2-34 \\\\ 8x_1+10x_2-38\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the Hessian definition:\n",
    "\n",
    "$$\\bf H_f = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2f}{\\partial x_1^2} & \\frac{\\partial^2f}{\\partial x_1\\partial x_2} \\\\\n",
    "\\frac{\\partial^2f}{\\partial x_2\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "So our second partials are as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial^2f}{\\partial x_1^2} &= 10 \\\\\n",
    "\\frac{\\partial^2f}{\\partial x_2\\partial x_1} &= 8 \\\\\n",
    "\\frac{\\partial^2f}{\\partial x_2\\partial x_1} &=  8\\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_2^2} &= 10\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "This makes the Hessian:\n",
    "$$\n",
    "\\bf {H}_f = \\begin{bmatrix} 10 &8 \\\\ 8 &10\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def booth(x):\n",
    "    return (x[0] + 2*x[1] - 7)**2 + (2*x[0] + x[1] - 5)**2\n",
    "\n",
    "def booth_gradient(x):\n",
    "    return np.array([10*x[0] + 8*x[1] - 34, 8*x[0] + 10*x[1] - 38])\n",
    "\n",
    "def booth_hessian(x):\n",
    "    return np.array([[10, 8], [8, 10]])\n",
    "\n",
    "booth_optimization_methods = {'newton-cg':{'function evals': [], 'function iters': []}, \n",
    "'nelder-mead':{'function evals': [], 'function iters': []}, \n",
    "'powell':{'function evals': [], 'function iters': []},\n",
    "'bfgs':{'function evals': [], 'function iters': []}, \n",
    "'dogleg':{'function evals': [], 'function iters': []}}\n",
    "\n",
    "starting_points = [np.random.randint(-20,20, size = (1,2)) for i in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: <span style=\"color: tan;\">Newton-CG</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "Test Run 1 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[15 -6]]\n",
      "The Minimum Occurs at (x, y) =  [1. 3.]\n",
      "The Minimum Value =  446.0\n",
      "Other Statistics:\n",
      "     fun: 6.310887241768095e-30\n",
      "     jac: array([0., 0.])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 4\n",
      "    nhev: 4\n",
      "     nit: 4\n",
      "    njev: 4\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([1., 3.])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "==========================================================\n",
      "Test Run 2 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[16  0]]\n",
      "The Minimum Occurs at (x, y) =  [1. 3.]\n",
      "The Minimum Value =  446.0\n",
      "Other Statistics:\n",
      "     fun: 7.099748146989106e-30\n",
      "     jac: array([1.42108547e-14, 7.10542736e-15])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 3\n",
      "    nhev: 3\n",
      "     nit: 3\n",
      "    njev: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([1., 3.])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "==========================================================\n",
      "Test Run 3 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[ 0 16]]\n",
      "The Minimum Occurs at (x, y) =  [1. 3.]\n",
      "The Minimum Value =  446.0\n",
      "Other Statistics:\n",
      "     fun: 3.549874073494553e-29\n",
      "     jac: array([1.42108547e-14, 0.00000000e+00])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 3\n",
      "    nhev: 3\n",
      "     nit: 3\n",
      "    njev: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([1., 3.])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "Summary Statistics of the Method:\n",
      "\n",
      "Function Evaluations:\n",
      "Minimum Function Evaluations: 3\n",
      "Maximum Function Evaluations: 4\n",
      "Mean Function Evaluations: 3.3333333333333335\n",
      "\n",
      "Function Iterations:\n",
      "Minimum Function Iterations:  3\n",
      "Maximum Function Iterations:  4\n",
      "Mean Function Iterations:  3.3333333333333335\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in enumerate(starting_points):\n",
    "    result = minimize(booth, x[1], method = 'Newton-CG',jac = booth_gradient, hess = booth_hessian, tol = 1.e-7) \n",
    "    \n",
    "    print(section_break)\n",
    "    print('Test Run', x[0] + 1, ':')\n",
    "    print(section_break)\n",
    "\n",
    "    print('Starting Value Used: ', x[1])\n",
    "    print(\"The Minimum Occurs at (x, y) = \", result.x)\n",
    "    print(\"The Minimum Value = \", f(result.x).round(3))\n",
    "    \n",
    "    print(\"Other Statistics:\") \n",
    "    print(result) \n",
    "    print(section_break)\n",
    "    print(section_break)\n",
    "    print('\\n')\n",
    "\n",
    "   \n",
    "    booth_optimization_methods['newton-cg']['function evals'].append(result['nfev'])\n",
    "    booth_optimization_methods['newton-cg']['function iters'].append(result['nit'])\n",
    "    \n",
    "\n",
    "# Displaying a summary of both function evaluations and iterations\n",
    "print(f\"\\\n",
    "Summary Statistics of the Method:\\n\\n\\\n",
    "Function Evaluations:\\n\\\n",
    "Minimum Function Evaluations: {min(booth_optimization_methods['newton-cg']['function evals'])}\\n\\\n",
    "Maximum Function Evaluations: {max(booth_optimization_methods['newton-cg']['function evals'])}\\n\\\n",
    "Mean Function Evaluations: {mean(booth_optimization_methods['newton-cg']['function evals'])}\\n\\n\\\n",
    "Function Iterations:\\n\\\n",
    "Minimum Function Iterations:  {min(booth_optimization_methods['newton-cg']['function iters'])}\\n\\\n",
    "Maximum Function Iterations:  {max(booth_optimization_methods['newton-cg']['function iters'])}\\n\\\n",
    "Mean Function Iterations:  {mean(booth_optimization_methods['newton-cg']['function iters'])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: <span style=\"color: tan;\">Nelder-Mead</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "Test Run 1 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[15 -6]]\n",
      "The Minimum Occurs at (x, y) =  [1.         3.00000002]\n",
      "The Minimum Value =  446.0\n",
      "Other Statistics:\n",
      " final_simplex: (array([[1.        , 3.00000002],\n",
      "       [1.00000002, 2.99999995],\n",
      "       [0.99999995, 3.00000001]]), array([1.75158283e-15, 4.38330322e-15, 7.66560524e-15]))\n",
      "           fun: 1.7515828349790307e-15\n",
      "       message: 'Optimization terminated successfully.'\n",
      "          nfev: 147\n",
      "           nit: 77\n",
      "        status: 0\n",
      "       success: True\n",
      "             x: array([1.        , 3.00000002])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "==========================================================\n",
      "Test Run 2 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[16  0]]\n",
      "The Minimum Occurs at (x, y) =  [0.99999995 3.00000003]\n",
      "The Minimum Value =  446.0\n",
      "Other Statistics:\n",
      " final_simplex: (array([[0.99999995, 3.00000003],\n",
      "       [1.00000004, 3.        ],\n",
      "       [0.99999999, 2.99999995]]), array([3.83493244e-15, 8.41918308e-15, 1.63733015e-14]))\n",
      "           fun: 3.834932441127539e-15\n",
      "       message: 'Optimization terminated successfully.'\n",
      "          nfev: 189\n",
      "           nit: 99\n",
      "        status: 0\n",
      "       success: True\n",
      "             x: array([0.99999995, 3.00000003])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "==========================================================\n",
      "Test Run 3 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[ 0 16]]\n",
      "The Minimum Occurs at (x, y) =  [1.00000001 2.99999997]\n",
      "The Minimum Value =  446.0\n",
      "Other Statistics:\n",
      " final_simplex: (array([[1.00000001, 2.99999997],\n",
      "       [1.00000007, 2.99999995],\n",
      "       [1.00000001, 3.00000006]]), array([2.13851400e-15, 1.05385176e-14, 2.09993782e-14]))\n",
      "           fun: 2.1385140016864933e-15\n",
      "       message: 'Optimization terminated successfully.'\n",
      "          nfev: 175\n",
      "           nit: 93\n",
      "        status: 0\n",
      "       success: True\n",
      "             x: array([1.00000001, 2.99999997])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "Summary Statistics of the Method:\n",
      "\n",
      "Function Evaluations:\n",
      "Minimum Function Evaluations: 147\n",
      "Maximum Function Evaluations: 189\n",
      "Mean Function Evaluations: 170.33333333333334\n",
      "\n",
      "Function Iterations:\n",
      "Minimum Function Iterations:  77\n",
      "Maximum Function Iterations:  99\n",
      "Mean Function Iterations:  89.66666666666667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in enumerate(starting_points):\n",
    "    result = minimize(booth, x[1], method = 'Nelder-Mead', tol = 1.e-7) \n",
    "    \n",
    "    print(section_break)\n",
    "    print('Test Run', x[0] + 1, ':')\n",
    "    print(section_break)\n",
    "\n",
    "    print('Starting Value Used: ', x[1])\n",
    "    print(\"The Minimum Occurs at (x, y) = \", result.x)\n",
    "    print(\"The Minimum Value = \", f(result.x).round(3))\n",
    "    \n",
    "    print(\"Other Statistics:\") \n",
    "    print(result) \n",
    "    print(section_break)\n",
    "    print(section_break)\n",
    "    print('\\n')\n",
    "\n",
    "   \n",
    "    booth_optimization_methods['nelder-mead']['function evals'].append(result['nfev'])\n",
    "    booth_optimization_methods['nelder-mead']['function iters'].append(result['nit'])\n",
    "    \n",
    "\n",
    "# Displaying a summary of both function evaluations and iterations\n",
    "print(f\"\\\n",
    "Summary Statistics of the Method:\\n\\n\\\n",
    "Function Evaluations:\\n\\\n",
    "Minimum Function Evaluations: {min(booth_optimization_methods['nelder-mead']['function evals'])}\\n\\\n",
    "Maximum Function Evaluations: {max(booth_optimization_methods['nelder-mead']['function evals'])}\\n\\\n",
    "Mean Function Evaluations: {mean(booth_optimization_methods['nelder-mead']['function evals'])}\\n\\n\\\n",
    "Function Iterations:\\n\\\n",
    "Minimum Function Iterations:  {min(booth_optimization_methods['nelder-mead']['function iters'])}\\n\\\n",
    "Maximum Function Iterations:  {max(booth_optimization_methods['nelder-mead']['function iters'])}\\n\\\n",
    "Mean Function Iterations:  {mean(booth_optimization_methods['nelder-mead']['function iters'])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: <span style=\"color: tan;\">Powell</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "Test Run 1 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[15 -6]]\n",
      "The Minimum Occurs at (x, y) =  [1. 3.]\n",
      "The Minimum Value =  446.0\n",
      "Other Statistics:\n",
      "   direc: array([[-5.57813687,  2.65781815],\n",
      "       [-0.08229579,  0.1574094 ]])\n",
      "     fun: 7.888609052210118e-31\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 94\n",
      "     nit: 4\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([1., 3.])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "==========================================================\n",
      "Test Run 2 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[16  0]]\n",
      "The Minimum Occurs at (x, y) =  [1. 3.]\n",
      "The Minimum Value =  446.0\n",
      "Other Statistics:\n",
      "   direc: array([[-0.99287054,  0.08510319],\n",
      "       [-0.483704  ,  0.63075002]])\n",
      "     fun: 3.944304526105059e-30\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 93\n",
      "     nit: 4\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([1., 3.])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "==========================================================\n",
      "Test Run 3 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[ 0 16]]\n",
      "The Minimum Occurs at (x, y) =  [1. 3.]\n",
      "The Minimum Value =  446.0\n",
      "Other Statistics:\n",
      "   direc: array([[ 0.    ,  1.    ],\n",
      "       [ 6.656 , -5.3248]])\n",
      "     fun: 7.967495142732219e-28\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 68\n",
      "     nit: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([1., 3.])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "Summary Statistics of the Method:\n",
      "\n",
      "Function Evaluations:\n",
      "Minimum Function Evaluations: 68\n",
      "Maximum Function Evaluations: 94\n",
      "Mean Function Evaluations: 85\n",
      "\n",
      "Function Iterations:\n",
      "Minimum Function Iterations:  3\n",
      "Maximum Function Iterations:  4\n",
      "Mean Function Iterations:  3.6666666666666665\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in enumerate(starting_points):\n",
    "    result = minimize(booth, x[1], method = 'Powell',tol = 1.e-7) \n",
    "    \n",
    "    print(section_break)\n",
    "    print('Test Run', x[0] + 1, ':')\n",
    "    print(section_break)\n",
    "\n",
    "    print('Starting Value Used: ', x[1])\n",
    "    print(\"The Minimum Occurs at (x, y) = \", result.x)\n",
    "    print(\"The Minimum Value = \", f(result.x).round(3))\n",
    "    \n",
    "    print(\"Other Statistics:\") \n",
    "    print(result) \n",
    "\n",
    "    print(section_break)\n",
    "    print(section_break)\n",
    "    print('\\n')\n",
    "\n",
    "    booth_optimization_methods['powell']['function evals'].append(result['nfev'])\n",
    "    booth_optimization_methods['powell']['function iters'].append(result['nit'])\n",
    "\n",
    "# Displaying a summary of both function evaluations and iterations\n",
    "print(f\"\\\n",
    "Summary Statistics of the Method:\\n\\n\\\n",
    "Function Evaluations:\\n\\\n",
    "Minimum Function Evaluations: {min(booth_optimization_methods['powell']['function evals'])}\\n\\\n",
    "Maximum Function Evaluations: {max(booth_optimization_methods['powell']['function evals'])}\\n\\\n",
    "Mean Function Evaluations: {mean(booth_optimization_methods['powell']['function evals'])}\\n\\n\\\n",
    "Function Iterations:\\n\\\n",
    "Minimum Function Iterations:  {min(booth_optimization_methods['powell']['function iters'])}\\n\\\n",
    "Maximum Function Iterations:  {max(booth_optimization_methods['powell']['function iters'])}\\n\\\n",
    "Mean Function Iterations:  {mean(booth_optimization_methods['powell']['function iters'])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 4: <span style=\"color: tan;\">BFGS</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "Test Run 1 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[15 -6]]\n",
      "The Minimum Occurs at (x, y) =  [1. 3.]\n",
      "The Minimum Value =  446.0\n",
      "Other Statistics:\n",
      "      fun: 3.202414184447051e-16\n",
      " hess_inv: array([[ 0.27807148, -0.22242848],\n",
      "       [-0.22242848,  0.27792263]])\n",
      "      jac: array([-2.24431142e-09, -5.00508079e-10])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 24\n",
      "      nit: 7\n",
      "     njev: 8\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([1., 3.])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "==========================================================\n",
      "Test Run 2 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[16  0]]\n",
      "The Minimum Occurs at (x, y) =  [0.99999999 3.        ]\n",
      "The Minimum Value =  446.0\n",
      "Other Statistics:\n",
      "      fun: 1.164005901269632e-16\n",
      " hess_inv: array([[ 0.27804317, -0.22288961],\n",
      "       [-0.22288961,  0.27945608]])\n",
      "      jac: array([3.21505629e-08, 5.44872591e-08])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 30\n",
      "      nit: 9\n",
      "     njev: 10\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([0.99999999, 3.        ])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "==========================================================\n",
      "Test Run 3 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[ 0 16]]\n",
      "The Minimum Occurs at (x, y) =  [1.         2.99999999]\n",
      "The Minimum Value =  446.0\n",
      "Other Statistics:\n",
      "      fun: 6.686747095419817e-17\n",
      " hess_inv: array([[ 0.27881844, -0.22243456],\n",
      "       [-0.22243456,  0.2778211 ]])\n",
      "      jac: array([6.0155056e-08, 4.2843384e-08])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 30\n",
      "      nit: 9\n",
      "     njev: 10\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([1.        , 2.99999999])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "Summary Statistics of the Method:\n",
      "\n",
      "Function Evaluations:\n",
      "Minimum Function Evaluations: 24\n",
      "Maximum Function Evaluations: 30\n",
      "Mean Function Evaluations: 28\n",
      "\n",
      "Function Iterations:\n",
      "Minimum Function Iterations:  7\n",
      "Maximum Function Iterations:  9\n",
      "Mean Function Iterations:  8.333333333333334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in enumerate(starting_points):\n",
    "    result = minimize(booth, x[1], method = 'BFGS',tol = 1.e-7) \n",
    "    \n",
    "    print(section_break)\n",
    "    print('Test Run', x[0] + 1, ':')\n",
    "    print(section_break)\n",
    "\n",
    "    print('Starting Value Used: ', x[1])\n",
    "    print(\"The Minimum Occurs at (x, y) = \", result.x)\n",
    "    print(\"The Minimum Value = \", f(result.x).round(3))\n",
    "    \n",
    "    print(\"Other Statistics:\") \n",
    "    print(result) \n",
    "\n",
    "    print(section_break)\n",
    "    print(section_break)\n",
    "    print('\\n')\n",
    "\n",
    "    booth_optimization_methods['bfgs']['function evals'].append(result['nfev'])\n",
    "    booth_optimization_methods['bfgs']['function iters'].append(result['nit'])\n",
    "\n",
    "# Displaying a summary of both function evaluations and iterations\n",
    "print(f\"\\\n",
    "Summary Statistics of the Method:\\n\\n\\\n",
    "Function Evaluations:\\n\\\n",
    "Minimum Function Evaluations: {min(booth_optimization_methods['bfgs']['function evals'])}\\n\\\n",
    "Maximum Function Evaluations: {max(booth_optimization_methods['bfgs']['function evals'])}\\n\\\n",
    "Mean Function Evaluations: {mean(booth_optimization_methods['bfgs']['function evals'])}\\n\\n\\\n",
    "Function Iterations:\\n\\\n",
    "Minimum Function Iterations:  {min(booth_optimization_methods['bfgs']['function iters'])}\\n\\\n",
    "Maximum Function Iterations:  {max(booth_optimization_methods['bfgs']['function iters'])}\\n\\\n",
    "Mean Function Iterations:  {mean(booth_optimization_methods['bfgs']['function iters'])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 5: <span style=\"color: tan;\">Dogleg</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "Test Run 1 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[15 -6]]\n",
      "The Minimum Occurs at (x, y) =  [1. 3.]\n",
      "The Minimum Value =  446.0\n",
      "Other Statistics:\n",
      "     fun: 0.0\n",
      "    hess: array([[10,  8],\n",
      "       [ 8, 10]])\n",
      "     jac: array([0., 0.])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 6\n",
      "    nhev: 5\n",
      "     nit: 5\n",
      "    njev: 6\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([1., 3.])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "==========================================================\n",
      "Test Run 2 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[16  0]]\n",
      "The Minimum Occurs at (x, y) =  [1. 3.]\n",
      "The Minimum Value =  446.0\n",
      "Other Statistics:\n",
      "     fun: 3.1554436208840472e-30\n",
      "    hess: array([[10,  8],\n",
      "       [ 8, 10]])\n",
      "     jac: array([7.10542736e-15, 0.00000000e+00])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 6\n",
      "    nhev: 5\n",
      "     nit: 5\n",
      "    njev: 6\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([1., 3.])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "==========================================================\n",
      "Test Run 3 :\n",
      "==========================================================\n",
      "Starting Value Used:  [[ 0 16]]\n",
      "The Minimum Occurs at (x, y) =  [1. 3.]\n",
      "The Minimum Value =  446.0\n",
      "Other Statistics:\n",
      "     fun: 1.5777218104420236e-30\n",
      "    hess: array([[10,  8],\n",
      "       [ 8, 10]])\n",
      "     jac: array([0., 0.])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 6\n",
      "    nhev: 5\n",
      "     nit: 5\n",
      "    njev: 6\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([1., 3.])\n",
      "==========================================================\n",
      "==========================================================\n",
      "\n",
      "\n",
      "Summary Statistics of the Method:\n",
      "\n",
      "Function Evaluations:\n",
      "Minimum Function Evaluations: 6\n",
      "Maximum Function Evaluations: 6\n",
      "Mean Function Evaluations: 6\n",
      "\n",
      "Function Iterations:\n",
      "Minimum Function Iterations:  5\n",
      "Maximum Function Iterations:  5\n",
      "Mean Function Iterations:  5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in enumerate(starting_points):\n",
    "    result = minimize(booth, x[1], method = 'dogleg',jac = booth_gradient, hess = booth_hessian, tol = 1.e-7) \n",
    "    \n",
    "    print(section_break)\n",
    "    print('Test Run', x[0] + 1, ':')\n",
    "    print(section_break)\n",
    "\n",
    "    print('Starting Value Used: ', x[1])\n",
    "    print(\"The Minimum Occurs at (x, y) = \", result.x)\n",
    "    print(\"The Minimum Value = \", f(result.x).round(3))\n",
    "    \n",
    "    print(\"Other Statistics:\") \n",
    "    print(result) \n",
    "    print(section_break)\n",
    "    print(section_break)\n",
    "    print('\\n')\n",
    "\n",
    "   \n",
    "    booth_optimization_methods['dogleg']['function evals'].append(result['nfev'])\n",
    "    booth_optimization_methods['dogleg']['function iters'].append(result['nit'])\n",
    "    \n",
    "\n",
    "# Displaying a summary of both function evaluations and iterations\n",
    "print(f\"\\\n",
    "Summary Statistics of the Method:\\n\\n\\\n",
    "Function Evaluations:\\n\\\n",
    "Minimum Function Evaluations: {min(booth_optimization_methods['dogleg']['function evals'])}\\n\\\n",
    "Maximum Function Evaluations: {max(booth_optimization_methods['dogleg']['function evals'])}\\n\\\n",
    "Mean Function Evaluations: {mean(booth_optimization_methods['dogleg']['function evals'])}\\n\\n\\\n",
    "Function Iterations:\\n\\\n",
    "Minimum Function Iterations:  {min(booth_optimization_methods['dogleg']['function iters'])}\\n\\\n",
    "Maximum Function Iterations:  {max(booth_optimization_methods['dogleg']['function iters'])}\\n\\\n",
    "Mean Function Iterations:  {mean(booth_optimization_methods['dogleg']['function iters'])}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the `dogleg` method is the most efficient method at find the minimum with fewest of every summary statistic escribed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
